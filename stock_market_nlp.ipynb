{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a28e558a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora, models, matutils\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b4874ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Label</th>\n",
       "      <th>Top1</th>\n",
       "      <th>Top2</th>\n",
       "      <th>Top3</th>\n",
       "      <th>Top4</th>\n",
       "      <th>Top5</th>\n",
       "      <th>Top6</th>\n",
       "      <th>Top7</th>\n",
       "      <th>Top8</th>\n",
       "      <th>...</th>\n",
       "      <th>Top16</th>\n",
       "      <th>Top17</th>\n",
       "      <th>Top18</th>\n",
       "      <th>Top19</th>\n",
       "      <th>Top20</th>\n",
       "      <th>Top21</th>\n",
       "      <th>Top22</th>\n",
       "      <th>Top23</th>\n",
       "      <th>Top24</th>\n",
       "      <th>Top25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-01-03</td>\n",
       "      <td>0</td>\n",
       "      <td>A 'hindrance to operations': extracts from the...</td>\n",
       "      <td>Scorecard</td>\n",
       "      <td>Hughes' instant hit buoys Blues</td>\n",
       "      <td>Jack gets his skates on at ice-cold Alex</td>\n",
       "      <td>Chaos as Maracana builds up for United</td>\n",
       "      <td>Depleted Leicester prevail as Elliott spoils E...</td>\n",
       "      <td>Hungry Spurs sense rich pickings</td>\n",
       "      <td>Gunners so wide of an easy target</td>\n",
       "      <td>...</td>\n",
       "      <td>Flintoff injury piles on woe for England</td>\n",
       "      <td>Hunters threaten Jospin with new battle of the...</td>\n",
       "      <td>Kohl's successor drawn into scandal</td>\n",
       "      <td>The difference between men and women</td>\n",
       "      <td>Sara Denver, nurse turned solicitor</td>\n",
       "      <td>Diana's landmine crusade put Tories in a panic</td>\n",
       "      <td>Yeltsin's resignation caught opposition flat-f...</td>\n",
       "      <td>Russian roulette</td>\n",
       "      <td>Sold out</td>\n",
       "      <td>Recovering a title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000-01-04</td>\n",
       "      <td>0</td>\n",
       "      <td>Scorecard</td>\n",
       "      <td>The best lake scene</td>\n",
       "      <td>Leader: German sleaze inquiry</td>\n",
       "      <td>Cheerio, boyo</td>\n",
       "      <td>The main recommendations</td>\n",
       "      <td>Has Cubie killed fees?</td>\n",
       "      <td>Has Cubie killed fees?</td>\n",
       "      <td>Has Cubie killed fees?</td>\n",
       "      <td>...</td>\n",
       "      <td>On the critical list</td>\n",
       "      <td>The timing of their lives</td>\n",
       "      <td>Dear doctor</td>\n",
       "      <td>Irish court halts IRA man's extradition to Nor...</td>\n",
       "      <td>Burundi peace initiative fades after rebels re...</td>\n",
       "      <td>PE points the way forward to the ECB</td>\n",
       "      <td>Campaigners keep up pressure on Nazi war crime...</td>\n",
       "      <td>Jane Ratcliffe</td>\n",
       "      <td>Yet more things you wouldn't know without the ...</td>\n",
       "      <td>Millennium bug fails to bite</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date Label                                               Top1  \\\n",
       "0  2000-01-03     0  A 'hindrance to operations': extracts from the...   \n",
       "1  2000-01-04     0                                          Scorecard   \n",
       "\n",
       "                  Top2                             Top3  \\\n",
       "0            Scorecard  Hughes' instant hit buoys Blues   \n",
       "1  The best lake scene    Leader: German sleaze inquiry   \n",
       "\n",
       "                                       Top4  \\\n",
       "0  Jack gets his skates on at ice-cold Alex   \n",
       "1                             Cheerio, boyo   \n",
       "\n",
       "                                     Top5  \\\n",
       "0  Chaos as Maracana builds up for United   \n",
       "1                The main recommendations   \n",
       "\n",
       "                                                Top6  \\\n",
       "0  Depleted Leicester prevail as Elliott spoils E...   \n",
       "1                             Has Cubie killed fees?   \n",
       "\n",
       "                               Top7                               Top8  ...  \\\n",
       "0  Hungry Spurs sense rich pickings  Gunners so wide of an easy target  ...   \n",
       "1            Has Cubie killed fees?             Has Cubie killed fees?  ...   \n",
       "\n",
       "                                      Top16  \\\n",
       "0  Flintoff injury piles on woe for England   \n",
       "1                      On the critical list   \n",
       "\n",
       "                                               Top17  \\\n",
       "0  Hunters threaten Jospin with new battle of the...   \n",
       "1                          The timing of their lives   \n",
       "\n",
       "                                 Top18  \\\n",
       "0  Kohl's successor drawn into scandal   \n",
       "1                          Dear doctor   \n",
       "\n",
       "                                               Top19  \\\n",
       "0               The difference between men and women   \n",
       "1  Irish court halts IRA man's extradition to Nor...   \n",
       "\n",
       "                                               Top20  \\\n",
       "0                Sara Denver, nurse turned solicitor   \n",
       "1  Burundi peace initiative fades after rebels re...   \n",
       "\n",
       "                                            Top21  \\\n",
       "0  Diana's landmine crusade put Tories in a panic   \n",
       "1            PE points the way forward to the ECB   \n",
       "\n",
       "                                               Top22             Top23  \\\n",
       "0  Yeltsin's resignation caught opposition flat-f...  Russian roulette   \n",
       "1  Campaigners keep up pressure on Nazi war crime...    Jane Ratcliffe   \n",
       "\n",
       "                                               Top24  \\\n",
       "0                                           Sold out   \n",
       "1  Yet more things you wouldn't know without the ...   \n",
       "\n",
       "                          Top25  \n",
       "0            Recovering a title  \n",
       "1  Millennium bug fails to bite  \n",
       "\n",
       "[2 rows x 27 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('stock_data_nlp.csv',encoding= 'unicode_escape')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b861cf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hughes' instant hit buoys Blues\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5a9e55e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4262, 27)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7f95833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4262 entries, 0 to 4261\n",
      "Data columns (total 27 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Date    4262 non-null   object\n",
      " 1   Label   4214 non-null   object\n",
      " 2   Top1    4210 non-null   object\n",
      " 3   Top2    4202 non-null   object\n",
      " 4   Top3    4192 non-null   object\n",
      " 5   Top4    4169 non-null   object\n",
      " 6   Top5    4158 non-null   object\n",
      " 7   Top6    4151 non-null   object\n",
      " 8   Top7    4144 non-null   object\n",
      " 9   Top8    4135 non-null   object\n",
      " 10  Top9    4127 non-null   object\n",
      " 11  Top10   4118 non-null   object\n",
      " 12  Top11   4106 non-null   object\n",
      " 13  Top12   4100 non-null   object\n",
      " 14  Top13   4091 non-null   object\n",
      " 15  Top14   4083 non-null   object\n",
      " 16  Top15   4075 non-null   object\n",
      " 17  Top16   4071 non-null   object\n",
      " 18  Top17   4064 non-null   object\n",
      " 19  Top18   4054 non-null   object\n",
      " 20  Top19   4049 non-null   object\n",
      " 21  Top20   4040 non-null   object\n",
      " 22  Top21   4030 non-null   object\n",
      " 23  Top22   4016 non-null   object\n",
      " 24  Top23   4010 non-null   object\n",
      " 25  Top24   4006 non-null   object\n",
      " 26  Top25   4004 non-null   object\n",
      "dtypes: object(27)\n",
      "memory usage: 899.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e79b83ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1                                                                                                                                                                                                                                                                                                     2166\n",
       "0                                                                                                                                                                                                                                                                                                     1935\n",
       "b'FBI arrests 10 Russian Spies in U.S.'                                                                                                                                                                                                                                                                  1\n",
       "b'PHILIPPINES Key witness in the Maguindanao massacre case killed'                                                                                                                                                                                                                                       1\n",
       "b\"Assisted suicide: law to be decriminalised 'by back door' from next week\"                                                                                                                                                                                                                              1\n",
       "                                                                                                                                                                                                                                                                                                      ... \n",
       "b'Hacker infiltrates the Sky News website and tampered with a petition to support the proposed television debate between the main party leaders, renaming it \"The Windbag Debate Petition\". '                                                                                                            1\n",
       "b'MYANMAR About 5 billion dollars from Total and Chevron flow into juntas secret accounts'                                                                                                                                                                                                               1\n",
       "b'North Korea Close To Finalising Nuclear Enrichment'                                                                                                                                                                                                                                                    1\n",
       "b'The war on drugs is immoral idiocy: While Latin American countries decriminalise narcotics, Britain persists in prohibition that causes vast human suffering'                                                                                                                                          1\n",
       "b'This depression is similar to the Great Panic of 1873; The US and Europe are heading towards deflation, tens of millions will never work again. \"It is the victory of an orthodoxy [..] whose main tenet is that imposing suffering on other people is how you show leadership in tough times.\"'       1\n",
       "Name: Label, Length: 115, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3d0c198",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the date column\n",
    "#the stock close trend is directly affected by the news from the same day\n",
    "df=df.drop(['Date'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4b0fa0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Top1</th>\n",
       "      <th>Top2</th>\n",
       "      <th>Top3</th>\n",
       "      <th>Top4</th>\n",
       "      <th>Top5</th>\n",
       "      <th>Top6</th>\n",
       "      <th>Top7</th>\n",
       "      <th>Top8</th>\n",
       "      <th>Top9</th>\n",
       "      <th>...</th>\n",
       "      <th>Top16</th>\n",
       "      <th>Top17</th>\n",
       "      <th>Top18</th>\n",
       "      <th>Top19</th>\n",
       "      <th>Top20</th>\n",
       "      <th>Top21</th>\n",
       "      <th>Top22</th>\n",
       "      <th>Top23</th>\n",
       "      <th>Top24</th>\n",
       "      <th>Top25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A 'hindrance to operations': extracts from the...</td>\n",
       "      <td>Scorecard</td>\n",
       "      <td>Hughes' instant hit buoys Blues</td>\n",
       "      <td>Jack gets his skates on at ice-cold Alex</td>\n",
       "      <td>Chaos as Maracana builds up for United</td>\n",
       "      <td>Depleted Leicester prevail as Elliott spoils E...</td>\n",
       "      <td>Hungry Spurs sense rich pickings</td>\n",
       "      <td>Gunners so wide of an easy target</td>\n",
       "      <td>Derby raise a glass to Strupar's debut double</td>\n",
       "      <td>...</td>\n",
       "      <td>Flintoff injury piles on woe for England</td>\n",
       "      <td>Hunters threaten Jospin with new battle of the...</td>\n",
       "      <td>Kohl's successor drawn into scandal</td>\n",
       "      <td>The difference between men and women</td>\n",
       "      <td>Sara Denver, nurse turned solicitor</td>\n",
       "      <td>Diana's landmine crusade put Tories in a panic</td>\n",
       "      <td>Yeltsin's resignation caught opposition flat-f...</td>\n",
       "      <td>Russian roulette</td>\n",
       "      <td>Sold out</td>\n",
       "      <td>Recovering a title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Scorecard</td>\n",
       "      <td>The best lake scene</td>\n",
       "      <td>Leader: German sleaze inquiry</td>\n",
       "      <td>Cheerio, boyo</td>\n",
       "      <td>The main recommendations</td>\n",
       "      <td>Has Cubie killed fees?</td>\n",
       "      <td>Has Cubie killed fees?</td>\n",
       "      <td>Has Cubie killed fees?</td>\n",
       "      <td>Hopkins 'furious' at Foster's lack of Hannibal...</td>\n",
       "      <td>...</td>\n",
       "      <td>On the critical list</td>\n",
       "      <td>The timing of their lives</td>\n",
       "      <td>Dear doctor</td>\n",
       "      <td>Irish court halts IRA man's extradition to Nor...</td>\n",
       "      <td>Burundi peace initiative fades after rebels re...</td>\n",
       "      <td>PE points the way forward to the ECB</td>\n",
       "      <td>Campaigners keep up pressure on Nazi war crime...</td>\n",
       "      <td>Jane Ratcliffe</td>\n",
       "      <td>Yet more things you wouldn't know without the ...</td>\n",
       "      <td>Millennium bug fails to bite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Coventry caught on counter by Flo</td>\n",
       "      <td>United's rivals on the road to Rio</td>\n",
       "      <td>Thatcher issues defence before trial by video</td>\n",
       "      <td>Police help Smith lay down the law at Everton</td>\n",
       "      <td>Tale of Trautmann bears two more retellings</td>\n",
       "      <td>England on the rack</td>\n",
       "      <td>Pakistan retaliate with call for video of Walsh</td>\n",
       "      <td>Cullinan continues his Cape monopoly</td>\n",
       "      <td>McGrath puts India out of their misery</td>\n",
       "      <td>...</td>\n",
       "      <td>South Melbourne (Australia)</td>\n",
       "      <td>Necaxa (Mexico)</td>\n",
       "      <td>Real Madrid (Spain)</td>\n",
       "      <td>Raja Casablanca (Morocco)</td>\n",
       "      <td>Corinthians (Brazil)</td>\n",
       "      <td>Tony's pet project</td>\n",
       "      <td>Al Nassr (Saudi Arabia)</td>\n",
       "      <td>Ideal Holmes show</td>\n",
       "      <td>Pinochet leaves hospital after tests</td>\n",
       "      <td>Useful links</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Pilgrim knows how to progress</td>\n",
       "      <td>Thatcher facing ban</td>\n",
       "      <td>McIlroy calls for Irish fighting spirit</td>\n",
       "      <td>Leicester bin stadium blueprint</td>\n",
       "      <td>United braced for Mexican wave</td>\n",
       "      <td>Auntie back in fashion, even if the dress look...</td>\n",
       "      <td>Shoaib appeal goes to the top</td>\n",
       "      <td>Hussain hurt by 'shambles' but lays blame on e...</td>\n",
       "      <td>England's decade of disasters</td>\n",
       "      <td>...</td>\n",
       "      <td>Putin admits Yeltsin quit to give him a head s...</td>\n",
       "      <td>BBC worst hit as digital TV begins to bite</td>\n",
       "      <td>How much can you pay for...</td>\n",
       "      <td>Christmas glitches</td>\n",
       "      <td>Upending a table, Chopping a line and Scoring ...</td>\n",
       "      <td>Scientific evidence 'unreliable', defence claims</td>\n",
       "      <td>Fusco wins judicial review in extradition case</td>\n",
       "      <td>Rebels thwart Russian advance</td>\n",
       "      <td>Blair orders shake-up of failing NHS</td>\n",
       "      <td>Lessons of law's hard heart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Hitches and Horlocks</td>\n",
       "      <td>Beckham off but United survive</td>\n",
       "      <td>Breast cancer screening</td>\n",
       "      <td>Alan Parker</td>\n",
       "      <td>Guardian readers: are you all whingers?</td>\n",
       "      <td>Hollywood Beyond</td>\n",
       "      <td>Ashes and diamonds</td>\n",
       "      <td>Whingers - a formidable minority</td>\n",
       "      <td>Alan Parker - part two</td>\n",
       "      <td>...</td>\n",
       "      <td>Most everywhere:  UDIs</td>\n",
       "      <td>Most wanted:  Chloe lunettes</td>\n",
       "      <td>Return of the cane 'completely off the agenda'</td>\n",
       "      <td>From Sleepy Hollow to Greeneland</td>\n",
       "      <td>Blunkett outlines vision for over 11s</td>\n",
       "      <td>Embattled Dobson attacks 'play now, pay later'...</td>\n",
       "      <td>Doom and the Dome</td>\n",
       "      <td>What is the north-south divide?</td>\n",
       "      <td>Aitken released from jail</td>\n",
       "      <td>Gone aloft</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                               Top1  \\\n",
       "0     0  A 'hindrance to operations': extracts from the...   \n",
       "1     0                                          Scorecard   \n",
       "2     0                  Coventry caught on counter by Flo   \n",
       "3     1                      Pilgrim knows how to progress   \n",
       "4     1                               Hitches and Horlocks   \n",
       "\n",
       "                                 Top2  \\\n",
       "0                           Scorecard   \n",
       "1                 The best lake scene   \n",
       "2  United's rivals on the road to Rio   \n",
       "3                 Thatcher facing ban   \n",
       "4      Beckham off but United survive   \n",
       "\n",
       "                                            Top3  \\\n",
       "0                Hughes' instant hit buoys Blues   \n",
       "1                  Leader: German sleaze inquiry   \n",
       "2  Thatcher issues defence before trial by video   \n",
       "3        McIlroy calls for Irish fighting spirit   \n",
       "4                        Breast cancer screening   \n",
       "\n",
       "                                            Top4  \\\n",
       "0       Jack gets his skates on at ice-cold Alex   \n",
       "1                                  Cheerio, boyo   \n",
       "2  Police help Smith lay down the law at Everton   \n",
       "3                Leicester bin stadium blueprint   \n",
       "4                                    Alan Parker   \n",
       "\n",
       "                                          Top5  \\\n",
       "0       Chaos as Maracana builds up for United   \n",
       "1                     The main recommendations   \n",
       "2  Tale of Trautmann bears two more retellings   \n",
       "3               United braced for Mexican wave   \n",
       "4      Guardian readers: are you all whingers?   \n",
       "\n",
       "                                                Top6  \\\n",
       "0  Depleted Leicester prevail as Elliott spoils E...   \n",
       "1                             Has Cubie killed fees?   \n",
       "2                                England on the rack   \n",
       "3  Auntie back in fashion, even if the dress look...   \n",
       "4                                   Hollywood Beyond   \n",
       "\n",
       "                                              Top7  \\\n",
       "0                 Hungry Spurs sense rich pickings   \n",
       "1                           Has Cubie killed fees?   \n",
       "2  Pakistan retaliate with call for video of Walsh   \n",
       "3                    Shoaib appeal goes to the top   \n",
       "4                               Ashes and diamonds   \n",
       "\n",
       "                                                Top8  \\\n",
       "0                  Gunners so wide of an easy target   \n",
       "1                             Has Cubie killed fees?   \n",
       "2               Cullinan continues his Cape monopoly   \n",
       "3  Hussain hurt by 'shambles' but lays blame on e...   \n",
       "4                   Whingers - a formidable minority   \n",
       "\n",
       "                                                Top9  ...  \\\n",
       "0      Derby raise a glass to Strupar's debut double  ...   \n",
       "1  Hopkins 'furious' at Foster's lack of Hannibal...  ...   \n",
       "2             McGrath puts India out of their misery  ...   \n",
       "3                      England's decade of disasters  ...   \n",
       "4                             Alan Parker - part two  ...   \n",
       "\n",
       "                                               Top16  \\\n",
       "0           Flintoff injury piles on woe for England   \n",
       "1                               On the critical list   \n",
       "2                        South Melbourne (Australia)   \n",
       "3  Putin admits Yeltsin quit to give him a head s...   \n",
       "4                             Most everywhere:  UDIs   \n",
       "\n",
       "                                               Top17  \\\n",
       "0  Hunters threaten Jospin with new battle of the...   \n",
       "1                          The timing of their lives   \n",
       "2                                    Necaxa (Mexico)   \n",
       "3         BBC worst hit as digital TV begins to bite   \n",
       "4                       Most wanted:  Chloe lunettes   \n",
       "\n",
       "                                            Top18  \\\n",
       "0             Kohl's successor drawn into scandal   \n",
       "1                                     Dear doctor   \n",
       "2                             Real Madrid (Spain)   \n",
       "3                     How much can you pay for...   \n",
       "4  Return of the cane 'completely off the agenda'   \n",
       "\n",
       "                                               Top19  \\\n",
       "0               The difference between men and women   \n",
       "1  Irish court halts IRA man's extradition to Nor...   \n",
       "2                          Raja Casablanca (Morocco)   \n",
       "3                                 Christmas glitches   \n",
       "4                   From Sleepy Hollow to Greeneland   \n",
       "\n",
       "                                               Top20  \\\n",
       "0                Sara Denver, nurse turned solicitor   \n",
       "1  Burundi peace initiative fades after rebels re...   \n",
       "2                               Corinthians (Brazil)   \n",
       "3  Upending a table, Chopping a line and Scoring ...   \n",
       "4              Blunkett outlines vision for over 11s   \n",
       "\n",
       "                                               Top21  \\\n",
       "0     Diana's landmine crusade put Tories in a panic   \n",
       "1               PE points the way forward to the ECB   \n",
       "2                                 Tony's pet project   \n",
       "3   Scientific evidence 'unreliable', defence claims   \n",
       "4  Embattled Dobson attacks 'play now, pay later'...   \n",
       "\n",
       "                                               Top22  \\\n",
       "0  Yeltsin's resignation caught opposition flat-f...   \n",
       "1  Campaigners keep up pressure on Nazi war crime...   \n",
       "2                            Al Nassr (Saudi Arabia)   \n",
       "3     Fusco wins judicial review in extradition case   \n",
       "4                                  Doom and the Dome   \n",
       "\n",
       "                             Top23  \\\n",
       "0                 Russian roulette   \n",
       "1                   Jane Ratcliffe   \n",
       "2                Ideal Holmes show   \n",
       "3    Rebels thwart Russian advance   \n",
       "4  What is the north-south divide?   \n",
       "\n",
       "                                               Top24  \\\n",
       "0                                           Sold out   \n",
       "1  Yet more things you wouldn't know without the ...   \n",
       "2               Pinochet leaves hospital after tests   \n",
       "3               Blair orders shake-up of failing NHS   \n",
       "4                          Aitken released from jail   \n",
       "\n",
       "                          Top25  \n",
       "0            Recovering a title  \n",
       "1  Millennium bug fails to bite  \n",
       "2                  Useful links  \n",
       "3   Lessons of law's hard heart  \n",
       "4                    Gone aloft  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf742947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all 25 news of the same day into a long string for doc2vec and vectorize\n",
    "combine_news=[]\n",
    "\n",
    "for index,row in df.iterrows():\n",
    "    combine_news.append(' '.join(str(x) for x in row[1:]))\n",
    "\n",
    "df['news']=pd.DataFrame(combine_news)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a9222cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Scorecard The best lake scene Leader: German sleaze inquiry Cheerio, boyo The main recommendations Has Cubie killed fees? Has Cubie killed fees? Has Cubie killed fees? Hopkins 'furious' at Foster's lack of Hannibal appetite Has Cubie killed fees? A tale of two tails I say what I like and I like what I say Elbows, Eyes and Nipples Task force to assess risk of asteroid collision How I found myself at last On the critical list The timing of their lives Dear doctor Irish court halts IRA man's extradition to Northern Ireland Burundi peace initiative fades after rebels reject Mandela as mediator PE points the way forward to the ECB Campaigners keep up pressure on Nazi war crimes suspect Jane Ratcliffe Yet more things you wouldn't know without the movies Millennium bug fails to bite\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[1,'news']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f6d4de5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' A.I.', ' M.A.N.D.Y.', ' U.N.', ' U.S.', ' U.K.', ' S.A.',\n",
       "       ' U.S.C.', ' D.C.', ' N.J.', ' i.e.', ' P.I.', ' A.N.C.', ' a.m.',\n",
       "       ' A.K.A.', ' P.R.', ' R.I.', ' E.U.', ' H.I.V.', ' I.H.T.',\n",
       "       ' B.C.', ' J.P.', ' N.S.', 'crimese.g.', ' C.I.A.', ' p.m.',\n",
       "       'Ph.D.', ' N.Y.', ' U.A.E.', 'sq.m.', ' I.M.F.', ' y.o.', ' i.a.',\n",
       "       ' I.D.', ' M.A.', ' H.W.', ' O.K.', ' N.K.', ' B.S.', ' A.T.M.',\n",
       "       ' W.H.O.', ' N.S.A.', ' P.M.', ' F.B.I.', ' P.E.I.', ' a.k.a.',\n",
       "       ' S.E.', ' A.D.', ' T.B.', ' J.K.', ' L.G.B.T.'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find all the 2/3 character abbrevations from the string corpus. Convert them to full form. \n",
    "# Then we can remove the punctuations without worrying losing the meaning of the abbrevation words.\n",
    "\n",
    "# combine all the news into a very long string\n",
    "long_news_str = '   '.join(df.news)\n",
    "\n",
    "# find all the abbrevations of 2 and 3 characters\n",
    "def find_abbr(text):\n",
    "    abbr = []\n",
    "    for i in re.finditer(r\"([A-Za-z]+| )([A-Za-z]\\.){2,}\", text):\n",
    "        abbr.append(i.group())\n",
    "    df_abbr = pd.Series(abbr)\n",
    "    return df_abbr.unique()\n",
    "\n",
    "find_abbr(long_news_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af30d528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ready stop words for the tokenization function below\n",
    "mywords = ['breaking','whilst', 'say', 'says', 'today','yesterday', 'news', 'tomorrow','iii', 'ii', 'like', 'ha',]\n",
    "final_stop = stopwords.words('english') + mywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b96097af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_wordlist(text, remove_stop_words=True, lemma_words=True):\n",
    "    \n",
    "    ''' Clean each document into a list of words:\n",
    "    1. convert abbrevations to full words\n",
    "    2. tokenize the text\n",
    "    3. remove non-alphabetic characters and one-letter words, including numbers and punctuations\n",
    "    4. remove stop words\n",
    "    '''\n",
    "    # clean the text, convert only the abbrs that are meaningful\n",
    "    text = re.sub(r\" A.T.M. \", \" Automated Teller Machine \", text)\n",
    "    text = re.sub(r\" C.I.A. \", \" Central Intelligence Agency \", text)\n",
    "    text = re.sub(r\" D.C. \", \" District of columbia \", text)\n",
    "    text = re.sub(r\" E.U. \", \" Europian Union \", text)\n",
    "    text = re.sub(r\" F.B.I. \", \" Federal Bureau of Investigation \", text)\n",
    "    text = re.sub(r\" H.I.V. \", \" Human immunodeficiency virus \", text)\n",
    "    text = re.sub(r\" I.H.T. \", \" inheritance tax \", text)\n",
    "    text = re.sub(r\" I.M.F. \", \" International Monetary Fund \", text)\n",
    "    text = re.sub(r\" I.D. \", \" identification \", text)\n",
    "    text = re.sub(r\" L.G.B.T. \", \" minority \", text)\n",
    "    text = re.sub(r\" M.A. \", \" Massachusetts \", text)\n",
    "    text = re.sub(r\" N.J. \", \" new jersey \", text)\n",
    "    text = re.sub(r\" N.K. \", \" north korea \", text)\n",
    "    text = re.sub(r\" N.S.A. \", \" National Security Agency \", text)\n",
    "    text = re.sub(r\" N.Y. \", \" new york \", text)\n",
    "    text = re.sub(r\" P.E.I. \", \" Prince Edward Island \", text)\n",
    "    text = re.sub(r\" P.M. \", \" prime minister \", text)\n",
    "    text = re.sub(r\" P.R.C \", \" china \", text)\n",
    "    text = re.sub(r\" S.A. \", \" south africa \", text)\n",
    "    text = re.sub(r\" R.I. \", \" Rhode Island \", text)\n",
    "    text = re.sub(r\" U.A.E. \", \" United Arab Emirates \", text)\n",
    "    text = re.sub(r\" U.K. \", \" england \", text)\n",
    "    text = re.sub(r\" U.N. \", \" new jersey \", text)\n",
    "    text = re.sub(r\" U.S. \", \" america \", text)\n",
    "    text = re.sub(r\" U.S.C. \", \" university of south california \", text)\n",
    "    text = re.sub(r\" W.H.O \", \" world health organization \", text)\n",
    "    text = re.sub(r\" a.m. \", \" morning \", text)\n",
    "    text = re.sub(r\" p.m. \", \" afternoon \", text)\n",
    "    text = re.sub(r\" Ph.D. \", \" doctor of philosophy \", text)\n",
    "    text = re.sub(r\" sq.m. \", \" square meter \", text)\n",
    "    \n",
    "    # Tokenize the string into word tokens\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # further clean the tokens: split toekns like \"b'Russia\" which still have punctuations in the token\n",
    "    ls = []\n",
    "    for word in tokens:\n",
    "        if \"'\" in word:\n",
    "            ls = ls + word.split(\"'\")\n",
    "    tokens = tokens + ls\n",
    "    \n",
    "    # Optionally, shorten words to their stems\n",
    "    if lemma_words:\n",
    "        tokens = [WordNetLemmatizer().lemmatize(word) for word in tokens]\n",
    "    \n",
    "    #Remove one letter tokens & non-alphabetic tokens, such as punctuation, then lower the tokens\n",
    "    tokens = [word.lower() for word in tokens if (word.isalpha() and len(word)>1)]\n",
    "\n",
    "    # remove stop words  \n",
    "    if remove_stop_words:\n",
    "        tokens = [word for word in tokens if word not in final_stop]\n",
    "               \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f3d40bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert each document to list of words\n",
    "df.news = df.news.apply(lambda x: text_to_wordlist(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3db9221a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scorecard',\n",
       " 'best',\n",
       " 'lake',\n",
       " 'scene',\n",
       " 'leader',\n",
       " 'german',\n",
       " 'sleaze',\n",
       " 'inquiry',\n",
       " 'cheerio',\n",
       " 'boyo',\n",
       " 'main',\n",
       " 'recommendation',\n",
       " 'cubie',\n",
       " 'killed',\n",
       " 'fee',\n",
       " 'cubie',\n",
       " 'killed',\n",
       " 'fee',\n",
       " 'cubie',\n",
       " 'killed',\n",
       " 'fee',\n",
       " 'hopkins',\n",
       " 'foster',\n",
       " 'lack',\n",
       " 'hannibal',\n",
       " 'appetite',\n",
       " 'cubie',\n",
       " 'killed',\n",
       " 'fee',\n",
       " 'tale',\n",
       " 'two',\n",
       " 'tail',\n",
       " 'elbows',\n",
       " 'eyes',\n",
       " 'nipples',\n",
       " 'task',\n",
       " 'force',\n",
       " 'ass',\n",
       " 'risk',\n",
       " 'asteroid',\n",
       " 'collision',\n",
       " 'found',\n",
       " 'last',\n",
       " 'critical',\n",
       " 'list',\n",
       " 'timing',\n",
       " 'life',\n",
       " 'dear',\n",
       " 'doctor',\n",
       " 'irish',\n",
       " 'court',\n",
       " 'halt',\n",
       " 'ira',\n",
       " 'man',\n",
       " 'extradition',\n",
       " 'northern',\n",
       " 'ireland',\n",
       " 'burundi',\n",
       " 'peace',\n",
       " 'initiative',\n",
       " 'fade',\n",
       " 'rebel',\n",
       " 'reject',\n",
       " 'mandela',\n",
       " 'mediator',\n",
       " 'pe',\n",
       " 'point',\n",
       " 'way',\n",
       " 'forward',\n",
       " 'ecb',\n",
       " 'campaigners',\n",
       " 'keep',\n",
       " 'pressure',\n",
       " 'nazi',\n",
       " 'war',\n",
       " 'crime',\n",
       " 'suspect',\n",
       " 'jane',\n",
       " 'ratcliffe',\n",
       " 'yet',\n",
       " 'thing',\n",
       " 'would',\n",
       " 'know',\n",
       " 'without',\n",
       " 'movie',\n",
       " 'millennium',\n",
       " 'bug',\n",
       " 'fails',\n",
       " 'bite',\n",
       " 'furious']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.news[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "716a9a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save it in a binary format for future use\n",
    "with open('news_wordlists.pkl', 'wb') as f:\n",
    "    pickle.dump(df, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c13f755",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d251a488",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9b0d2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=api.load('text8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6999043",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "model = Word2Vec(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b60ade39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.509381   -0.16613127 -0.8992871  -2.9068344   1.4750314   0.80374986\n",
      "  0.7952001   0.7238951   1.6035595  -0.02354915  3.0688202   2.5213227\n",
      "  1.0252578  -0.05220444 -1.1082993  -1.2946367   0.66607404 -0.19928414\n",
      " -0.97570974  1.626208    1.2356777   0.7779477  -1.0477018   2.0057783\n",
      " -2.4510858   3.0472026  -1.0234712   1.9142127   0.14806604  2.4640114\n",
      " -0.23066552 -3.6230218   2.607124    1.1380025  -2.2711272  -2.235657\n",
      " -0.7185839   0.6535337   0.67939144  1.8852011   0.72671854 -0.70859224\n",
      "  3.2457647  -1.0079061   1.5675293  -1.6929959   0.32602417 -0.97245795\n",
      " -0.35630977 -1.1249006   2.1012487   2.1822665   1.8151661  -0.6936743\n",
      " -0.6782292   2.1006563   1.076847    1.4930735   0.6676121   0.17348084\n",
      " -0.8526461  -0.5471324   0.49434534  1.169552   -0.16901694  3.1406465\n",
      "  1.688444    1.5923692   0.3708412  -1.9049963  -0.8629914   0.90316373\n",
      "  0.7333295  -3.636362    0.48794597 -1.7392259  -0.21886991  2.5903301\n",
      " -0.15998559  1.3041346  -0.8568234  -1.6246411   0.22221313  0.79080755\n",
      "  0.1381255   0.5350115   0.45606962 -1.6162015  -2.1773198  -1.1138657\n",
      "  0.02572116  0.743301    1.5139221  -0.84940565  3.2244778  -0.98316795\n",
      " -0.1767803  -2.0406706  -1.8481307   4.058786  ]\n"
     ]
    }
   ],
   "source": [
    "economy=model.wv.get_vector('south')\n",
    "print(economy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "430f8e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def doc2vec(model,wordlist):\n",
    "    vector_list=[model.wv.get_vector(word) for word in wordlist if word in model.wv]\n",
    "    doc_vector=np.mean(vector_list,axis=0)\n",
    "    return doc_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "36eaf36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert each document (list of words) to a document vecter, then save into a list of doc_vec\n",
    "x_doc = [doc2vec(model, doc) for doc in df.news]  \n",
    "X_doc= pd.Series(x_doc, name = 'doc_vec') # list to df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15d89cbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4262,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_doc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5271c9de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be700cf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f3f8e29e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>news</th>\n",
       "      <th>doc_vec</th>\n",
       "      <th>news_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[operation, extract, leaked, report, scorecard...</td>\n",
       "      <td>[-0.13853689, -0.01842522, -0.011283746, 0.025...</td>\n",
       "      <td>operation extract leaked report scorecard hugh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[scorecard, best, lake, scene, leader, german,...</td>\n",
       "      <td>[-0.13274541, -0.32202816, 0.17546004, 0.15300...</td>\n",
       "      <td>scorecard best lake scene leader german sleaze...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[coventry, caught, counter, flo, united, rival...</td>\n",
       "      <td>[-0.41247106, 0.4019597, 0.16694483, 0.0331079...</td>\n",
       "      <td>coventry caught counter flo united rival road ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>[pilgrim, know, progress, thatcher, facing, ba...</td>\n",
       "      <td>[-0.232915, -0.15494952, 0.13468608, -0.152680...</td>\n",
       "      <td>pilgrim know progress thatcher facing ban mcil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>[hitches, horlocks, beckham, united, survive, ...</td>\n",
       "      <td>[-0.33469847, -0.034216043, 0.117651, -0.06944...</td>\n",
       "      <td>hitches horlocks beckham united survive breast...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                               news  \\\n",
       "0     0  [operation, extract, leaked, report, scorecard...   \n",
       "1     0  [scorecard, best, lake, scene, leader, german,...   \n",
       "2     0  [coventry, caught, counter, flo, united, rival...   \n",
       "3     1  [pilgrim, know, progress, thatcher, facing, ba...   \n",
       "4     1  [hitches, horlocks, beckham, united, survive, ...   \n",
       "\n",
       "                                             doc_vec  \\\n",
       "0  [-0.13853689, -0.01842522, -0.011283746, 0.025...   \n",
       "1  [-0.13274541, -0.32202816, 0.17546004, 0.15300...   \n",
       "2  [-0.41247106, 0.4019597, 0.16694483, 0.0331079...   \n",
       "3  [-0.232915, -0.15494952, 0.13468608, -0.152680...   \n",
       "4  [-0.33469847, -0.034216043, 0.117651, -0.06944...   \n",
       "\n",
       "                                            news_str  \n",
       "0  operation extract leaked report scorecard hugh...  \n",
       "1  scorecard best lake scene leader german sleaze...  \n",
       "2  coventry caught counter flo united rival road ...  \n",
       "3  pilgrim know progress thatcher facing ban mcil...  \n",
       "4  hitches horlocks beckham united survive breast...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a new dataframe containing only label, news, doc_vec\n",
    "newdf = pd.concat([df.Label,df.news,X_doc], axis = 1)\n",
    "\n",
    "# convert each row in the news column from a list of tokens to a string\n",
    "tokenstrlist = []\n",
    "for tokenlist in  df.news:\n",
    "    tokenstr = ' '.join(tokenlist)\n",
    "    tokenstrlist.append(tokenstr)\n",
    "\n",
    "# add the string format new as clolumn 'news_str' to the data frame\n",
    "newdf['news_str'] = pd.Series(tokenstrlist)\n",
    "\n",
    "# the final data frame is ready for modeling\n",
    "newdf.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f0830374",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf = newdf[newdf['Label'].isin(['0', '1'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c66a9226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('O')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdf['Label'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6b48032a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "newdf['Label'] = newdf['Label'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f8b507d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('label_news_docvec_newsstr.pkl', 'wb') as f:\n",
    "    pickle.dump(newdf, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40670391",
   "metadata": {},
   "source": [
    "### Finding the best baseline Model\n",
    "##### every dataset is different\n",
    "We dont know which model works best for our data until we try. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "201c50b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f61cf7cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>news</th>\n",
       "      <th>doc_vec</th>\n",
       "      <th>news_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[operation, extract, leaked, report, scorecard...</td>\n",
       "      <td>[-0.13853689, -0.01842522, -0.011283746, 0.025...</td>\n",
       "      <td>operation extract leaked report scorecard hugh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[scorecard, best, lake, scene, leader, german,...</td>\n",
       "      <td>[-0.13274541, -0.32202816, 0.17546004, 0.15300...</td>\n",
       "      <td>scorecard best lake scene leader german sleaze...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[coventry, caught, counter, flo, united, rival...</td>\n",
       "      <td>[-0.41247106, 0.4019597, 0.16694483, 0.0331079...</td>\n",
       "      <td>coventry caught counter flo united rival road ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>[pilgrim, know, progress, thatcher, facing, ba...</td>\n",
       "      <td>[-0.232915, -0.15494952, 0.13468608, -0.152680...</td>\n",
       "      <td>pilgrim know progress thatcher facing ban mcil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>[hitches, horlocks, beckham, united, survive, ...</td>\n",
       "      <td>[-0.33469847, -0.034216043, 0.117651, -0.06944...</td>\n",
       "      <td>hitches horlocks beckham united survive breast...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label                                               news  \\\n",
       "0      0  [operation, extract, leaked, report, scorecard...   \n",
       "1      0  [scorecard, best, lake, scene, leader, german,...   \n",
       "2      0  [coventry, caught, counter, flo, united, rival...   \n",
       "3      1  [pilgrim, know, progress, thatcher, facing, ba...   \n",
       "4      1  [hitches, horlocks, beckham, united, survive, ...   \n",
       "\n",
       "                                             doc_vec  \\\n",
       "0  [-0.13853689, -0.01842522, -0.011283746, 0.025...   \n",
       "1  [-0.13274541, -0.32202816, 0.17546004, 0.15300...   \n",
       "2  [-0.41247106, 0.4019597, 0.16694483, 0.0331079...   \n",
       "3  [-0.232915, -0.15494952, 0.13468608, -0.152680...   \n",
       "4  [-0.33469847, -0.034216043, 0.117651, -0.06944...   \n",
       "\n",
       "                                            news_str  \n",
       "0  operation extract leaked report scorecard hugh...  \n",
       "1  scorecard best lake scene leader german sleaze...  \n",
       "2  coventry caught counter flo united rival road ...  \n",
       "3  pilgrim know progress thatcher facing ban mcil...  \n",
       "4  hitches horlocks beckham united survive breast...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('label_news_docvec_newsstr.pkl', 'rb') as r:\n",
    "    df = pickle.load(r)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0624c02b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.13853689, -0.01842522, -0.01128375,  0.02521024, -0.12980145,\n",
       "       -0.4068818 ,  0.13044144,  0.25541753,  0.37049475,  0.09962238,\n",
       "       -0.23451243, -0.27692914,  0.24458377,  0.31107265, -0.11690012,\n",
       "       -0.5110352 ,  0.0377483 , -0.4658377 ,  0.45661828, -0.18214466],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0,2][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "001d3a88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.13853689, -0.01842522, -0.01128375,  0.02521024, -0.12980145,\n",
       "       -0.4068818 ,  0.13044144,  0.25541753,  0.37049475,  0.09962238,\n",
       "       -0.23451243, -0.27692914,  0.24458377,  0.31107265, -0.11690012,\n",
       "       -0.5110352 ,  0.0377483 , -0.4658377 ,  0.45661828, -0.18214466],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0,2][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b6862010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'operation extract leaked report scorecard hughes instant hit buoy blues jack get skate alex chaos maracana build united depleted leicester prevail elliott spoil everton party hungry spurs sense rich picking gunners wide easy target derby raise glass strupar debut double southgate strike leeds pay penalty hammers hand robson youthful lesson saints party wear wolf turned lamb stump mike catch testy gough taunt langer escape hit flintoff injury pile woe england hunters threaten jospin new battle somme kohl successor drawn scandal difference men woman sara denver nurse turned solicitor diana landmine crusade put tories panic yeltsin resignation caught opposition russian roulette sold recovering title hindrance'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d9b3a3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=df.iloc[:3623,:] #85% for training and validation, 15% for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9121e2b0",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1776d250",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import  svm, naive_bayes, neighbors, ensemble\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "\n",
    "lr_model = LogisticRegression()\n",
    "nb_model = naive_bayes.GaussianNB()\n",
    "knn_model = neighbors.KNeighborsClassifier()\n",
    "svc_model = svm.SVC(probability=True, gamma=\"scale\")\n",
    "rf_model = ensemble.RandomForestClassifier(n_estimators=100)\n",
    "et_model = ensemble.ExtraTreesClassifier(n_estimators=100)\n",
    "ada_model = ensemble.AdaBoostClassifier()\n",
    "xgb_model = xgb.XGBClassifier(max_depth=50, n_estimators=80, learning_rate=0.1, colsample_bytree=.7, gamma=0, \n",
    "                              reg_alpha=4, objective='binary:logistic', eta=0.3, silent=1, subsample=0.8)\n",
    "\n",
    "models = [\"lr_model\", \"nb_model\", \"knn_model\", \"svc_model\", \"rf_model\", \"et_model\", \"ada_model\", \"xgb_model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "94535110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model_filter(modellist, X, y):\n",
    "    ''' 1. split the train data further into train and validation (17%). \n",
    "        2. fit the train data into each model of the model list\n",
    "        3. get the classification report based on the model performance on validation data\n",
    "    '''\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X,y, test_size = 0.17, random_state = 100)\n",
    "    for model_name in modellist:\n",
    "        curr_model = eval(model_name)\n",
    "        curr_model.fit(X_train, y_train) \n",
    "        print(f'{model_name} \\n report:{classification_report(y_valid, curr_model.predict(X_valid))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db2e9de",
   "metadata": {},
   "source": [
    "## Using Different word embedding techniques to filter the baseline models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b1906f",
   "metadata": {},
   "source": [
    "### Bag-Of-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "83297e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(df_train['news_str'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5d3807b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(analyzer='word')\n",
    "X = count_vect.fit_transform(df_train.news_str).toarray()\n",
    "y = df_train.Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a0cbe315",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harsh\\anaconda3\\envs\\mygpu\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr_model \n",
      " report:              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.44      0.45       284\n",
      "           1       0.53      0.54      0.54       332\n",
      "\n",
      "    accuracy                           0.50       616\n",
      "   macro avg       0.49      0.49      0.49       616\n",
      "weighted avg       0.49      0.50      0.49       616\n",
      "\n",
      "nb_model \n",
      " report:              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.58      0.53       284\n",
      "           1       0.57      0.47      0.51       332\n",
      "\n",
      "    accuracy                           0.52       616\n",
      "   macro avg       0.53      0.53      0.52       616\n",
      "weighted avg       0.53      0.52      0.52       616\n",
      "\n",
      "knn_model \n",
      " report:              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.18      0.26       284\n",
      "           1       0.55      0.84      0.66       332\n",
      "\n",
      "    accuracy                           0.54       616\n",
      "   macro avg       0.52      0.51      0.46       616\n",
      "weighted avg       0.52      0.54      0.48       616\n",
      "\n",
      "svc_model \n",
      " report:              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.23      0.32       284\n",
      "           1       0.55      0.79      0.65       332\n",
      "\n",
      "    accuracy                           0.53       616\n",
      "   macro avg       0.52      0.51      0.48       616\n",
      "weighted avg       0.52      0.53      0.49       616\n",
      "\n",
      "rf_model \n",
      " report:              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.32      0.39       284\n",
      "           1       0.56      0.73      0.63       332\n",
      "\n",
      "    accuracy                           0.54       616\n",
      "   macro avg       0.53      0.52      0.51       616\n",
      "weighted avg       0.53      0.54      0.52       616\n",
      "\n",
      "et_model \n",
      " report:              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.33      0.38       284\n",
      "           1       0.53      0.65      0.59       332\n",
      "\n",
      "    accuracy                           0.50       616\n",
      "   macro avg       0.49      0.49      0.48       616\n",
      "weighted avg       0.49      0.50      0.49       616\n",
      "\n",
      "ada_model \n",
      " report:              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.40      0.42       284\n",
      "           1       0.53      0.58      0.55       332\n",
      "\n",
      "    accuracy                           0.50       616\n",
      "   macro avg       0.49      0.49      0.49       616\n",
      "weighted avg       0.49      0.50      0.49       616\n",
      "\n",
      "[14:18:07] WARNING: C:\\Users\\dev-admin\\croot2\\xgboost-split_1675461376218\\work\\src\\learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "xgb_model \n",
      " report:              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.35      0.39       284\n",
      "           1       0.53      0.62      0.57       332\n",
      "\n",
      "    accuracy                           0.49       616\n",
      "   macro avg       0.48      0.48      0.48       616\n",
      "weighted avg       0.48      0.49      0.48       616\n",
      "\n"
     ]
    }
   ],
   "source": [
    "baseline_model_filter(models, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59557ee",
   "metadata": {},
   "source": [
    "## Word level TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3787c19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer(analyzer='word')\n",
    "X = tfidf_vect.fit_transform(df_train.news_str).toarray()\n",
    "y = df_train.Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eac4335",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fa1a259c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr_model \n",
      " report:              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.30      0.37       284\n",
      "           1       0.54      0.71      0.62       332\n",
      "\n",
      "    accuracy                           0.52       616\n",
      "   macro avg       0.51      0.51      0.49       616\n",
      "weighted avg       0.51      0.52      0.50       616\n",
      "\n",
      "nb_model \n",
      " report:              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.51      0.50       284\n",
      "           1       0.56      0.54      0.55       332\n",
      "\n",
      "    accuracy                           0.53       616\n",
      "   macro avg       0.52      0.52      0.52       616\n",
      "weighted avg       0.53      0.53      0.53       616\n",
      "\n",
      "knn_model \n",
      " report:              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.50      0.49       284\n",
      "           1       0.55      0.52      0.54       332\n",
      "\n",
      "    accuracy                           0.51       616\n",
      "   macro avg       0.51      0.51      0.51       616\n",
      "weighted avg       0.51      0.51      0.51       616\n",
      "\n",
      "svc_model \n",
      " report:              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.12      0.19       284\n",
      "           1       0.53      0.85      0.65       332\n",
      "\n",
      "    accuracy                           0.51       616\n",
      "   macro avg       0.47      0.48      0.42       616\n",
      "weighted avg       0.47      0.51      0.44       616\n",
      "\n",
      "rf_model \n",
      " report:              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.30      0.36       284\n",
      "           1       0.53      0.68      0.60       332\n",
      "\n",
      "    accuracy                           0.50       616\n",
      "   macro avg       0.49      0.49      0.48       616\n",
      "weighted avg       0.49      0.50      0.49       616\n",
      "\n",
      "et_model \n",
      " report:              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.37      0.42       284\n",
      "           1       0.56      0.67      0.61       332\n",
      "\n",
      "    accuracy                           0.53       616\n",
      "   macro avg       0.52      0.52      0.52       616\n",
      "weighted avg       0.53      0.53      0.52       616\n",
      "\n",
      "ada_model \n",
      " report:              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.27      0.35       284\n",
      "           1       0.54      0.73      0.62       332\n",
      "\n",
      "    accuracy                           0.52       616\n",
      "   macro avg       0.50      0.50      0.48       616\n",
      "weighted avg       0.51      0.52      0.49       616\n",
      "\n",
      "[14:39:21] WARNING: C:\\Users\\dev-admin\\croot2\\xgboost-split_1675461376218\\work\\src\\learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "xgb_model \n",
      " report:              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.32      0.37       284\n",
      "           1       0.52      0.61      0.56       332\n",
      "\n",
      "    accuracy                           0.48       616\n",
      "   macro avg       0.47      0.47      0.46       616\n",
      "weighted avg       0.47      0.48      0.47       616\n",
      "\n"
     ]
    }
   ],
   "source": [
    "baseline_model_filter(models, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfa111c",
   "metadata": {},
   "source": [
    "## Character Level TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6df25932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr_model \n",
      " report:              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.05      0.10       284\n",
      "           1       0.55      0.97      0.70       332\n",
      "\n",
      "    accuracy                           0.55       616\n",
      "   macro avg       0.59      0.51      0.40       616\n",
      "weighted avg       0.58      0.55      0.42       616\n",
      "\n",
      "nb_model \n",
      " report:              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.99      0.63       284\n",
      "           1       0.62      0.02      0.03       332\n",
      "\n",
      "    accuracy                           0.46       616\n",
      "   macro avg       0.54      0.50      0.33       616\n",
      "weighted avg       0.55      0.46      0.31       616\n",
      "\n",
      "knn_model \n",
      " report:              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.47      0.46       284\n",
      "           1       0.53      0.52      0.52       332\n",
      "\n",
      "    accuracy                           0.49       616\n",
      "   macro avg       0.49      0.49      0.49       616\n",
      "weighted avg       0.49      0.49      0.49       616\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harsh\\anaconda3\\envs\\mygpu\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\harsh\\anaconda3\\envs\\mygpu\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\harsh\\anaconda3\\envs\\mygpu\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svc_model \n",
      " report:              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       284\n",
      "           1       0.54      1.00      0.70       332\n",
      "\n",
      "    accuracy                           0.54       616\n",
      "   macro avg       0.27      0.50      0.35       616\n",
      "weighted avg       0.29      0.54      0.38       616\n",
      "\n",
      "rf_model \n",
      " report:              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.42      0.44       284\n",
      "           1       0.54      0.59      0.57       332\n",
      "\n",
      "    accuracy                           0.51       616\n",
      "   macro avg       0.51      0.51      0.51       616\n",
      "weighted avg       0.51      0.51      0.51       616\n",
      "\n",
      "et_model \n",
      " report:              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.43      0.45       284\n",
      "           1       0.54      0.58      0.56       332\n",
      "\n",
      "    accuracy                           0.51       616\n",
      "   macro avg       0.51      0.51      0.51       616\n",
      "weighted avg       0.51      0.51      0.51       616\n",
      "\n",
      "ada_model \n",
      " report:              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.37      0.40       284\n",
      "           1       0.52      0.59      0.55       332\n",
      "\n",
      "    accuracy                           0.49       616\n",
      "   macro avg       0.48      0.48      0.47       616\n",
      "weighted avg       0.48      0.49      0.48       616\n",
      "\n",
      "[14:42:01] WARNING: C:\\Users\\dev-admin\\croot2\\xgboost-split_1675461376218\\work\\src\\learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "xgb_model \n",
      " report:              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.45      0.45       284\n",
      "           1       0.54      0.54      0.54       332\n",
      "\n",
      "    accuracy                           0.50       616\n",
      "   macro avg       0.50      0.50      0.50       616\n",
      "weighted avg       0.50      0.50      0.50       616\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfidf_chars_vect = TfidfVectorizer(analyzer='char')\n",
    "\n",
    "X = tfidf_chars_vect.fit_transform(df_train.news_str).toarray()\n",
    "y = df_train.Label\n",
    "\n",
    "baseline_model_filter(models, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c49e0b",
   "metadata": {},
   "source": [
    "### Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a77a1820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr_model \n",
      " report:              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.32      0.38       284\n",
      "           1       0.55      0.70      0.61       332\n",
      "\n",
      "    accuracy                           0.52       616\n",
      "   macro avg       0.51      0.51      0.50       616\n",
      "weighted avg       0.51      0.52      0.51       616\n",
      "\n",
      "nb_model \n",
      " report:              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.42      0.44       284\n",
      "           1       0.54      0.57      0.55       332\n",
      "\n",
      "    accuracy                           0.50       616\n",
      "   macro avg       0.50      0.50      0.50       616\n",
      "weighted avg       0.50      0.50      0.50       616\n",
      "\n",
      "knn_model \n",
      " report:              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.39      0.43       284\n",
      "           1       0.55      0.63      0.59       332\n",
      "\n",
      "    accuracy                           0.52       616\n",
      "   macro avg       0.51      0.51      0.51       616\n",
      "weighted avg       0.51      0.52      0.51       616\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harsh\\AppData\\Local\\Temp\\ipykernel_25072\\3845775326.py:2: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  y = np.array(list(df.Label[:3623]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svc_model \n",
      " report:              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.13      0.20       284\n",
      "           1       0.53      0.84      0.65       332\n",
      "\n",
      "    accuracy                           0.51       616\n",
      "   macro avg       0.47      0.49      0.43       616\n",
      "weighted avg       0.48      0.51      0.44       616\n",
      "\n",
      "rf_model \n",
      " report:              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.42      0.45       284\n",
      "           1       0.56      0.62      0.59       332\n",
      "\n",
      "    accuracy                           0.53       616\n",
      "   macro avg       0.52      0.52      0.52       616\n",
      "weighted avg       0.52      0.53      0.52       616\n",
      "\n",
      "et_model \n",
      " report:              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.37      0.40       284\n",
      "           1       0.53      0.60      0.56       332\n",
      "\n",
      "    accuracy                           0.49       616\n",
      "   macro avg       0.48      0.48      0.48       616\n",
      "weighted avg       0.49      0.49      0.49       616\n",
      "\n",
      "ada_model \n",
      " report:              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.41      0.44       284\n",
      "           1       0.55      0.63      0.59       332\n",
      "\n",
      "    accuracy                           0.53       616\n",
      "   macro avg       0.52      0.52      0.52       616\n",
      "weighted avg       0.52      0.53      0.52       616\n",
      "\n",
      "[14:42:13] WARNING: C:\\Users\\dev-admin\\croot2\\xgboost-split_1675461376218\\work\\src\\learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "xgb_model \n",
      " report:              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.40      0.45       284\n",
      "           1       0.56      0.65      0.60       332\n",
      "\n",
      "    accuracy                           0.54       616\n",
      "   macro avg       0.53      0.53      0.52       616\n",
      "weighted avg       0.53      0.54      0.53       616\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = np.array(list(df_train.doc_vec))\n",
    "y = np.array(list(df.Label[:3623]))\n",
    "baseline_model_filter(models, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142f5a3c",
   "metadata": {},
   "source": [
    "### see performance on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fb4e55b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df.iloc[3623:,:]\n",
    "y_test = df_test.Label\n",
    "X_test = df_test.news_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5561a1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB() \n",
      " report:              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.47      0.49       229\n",
      "           1       0.55      0.60      0.58       249\n",
      "\n",
      "    accuracy                           0.54       478\n",
      "   macro avg       0.54      0.53      0.53       478\n",
      "weighted avg       0.54      0.54      0.54       478\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfidf_chars_vect = TfidfVectorizer(analyzer='word')\n",
    "train_news = tfidf_chars_vect.fit(df_train.news_str)\n",
    "X_train_trans = train_news.transform(df_train.news_str).toarray()\n",
    "X_test_trans = train_news.transform(X_test).toarray()\n",
    "\n",
    "y_train = df_train.Label\n",
    "\n",
    "nb_model.fit(X_train_trans, y_train) \n",
    "\n",
    "\n",
    "print(f'{nb_model} \\n report:{classification_report(y_test, nb_model.predict(X_test_trans))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a8a9ef85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>news</th>\n",
       "      <th>doc_vec</th>\n",
       "      <th>news_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[operation, extract, leaked, report, scorecard...</td>\n",
       "      <td>[-0.13853689, -0.01842522, -0.011283746, 0.025...</td>\n",
       "      <td>operation extract leaked report scorecard hugh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[scorecard, best, lake, scene, leader, german,...</td>\n",
       "      <td>[-0.13274541, -0.32202816, 0.17546004, 0.15300...</td>\n",
       "      <td>scorecard best lake scene leader german sleaze...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[coventry, caught, counter, flo, united, rival...</td>\n",
       "      <td>[-0.41247106, 0.4019597, 0.16694483, 0.0331079...</td>\n",
       "      <td>coventry caught counter flo united rival road ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>[pilgrim, know, progress, thatcher, facing, ba...</td>\n",
       "      <td>[-0.232915, -0.15494952, 0.13468608, -0.152680...</td>\n",
       "      <td>pilgrim know progress thatcher facing ban mcil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>[hitches, horlocks, beckham, united, survive, ...</td>\n",
       "      <td>[-0.33469847, -0.034216043, 0.117651, -0.06944...</td>\n",
       "      <td>hitches horlocks beckham united survive breast...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label                                               news  \\\n",
       "0      0  [operation, extract, leaked, report, scorecard...   \n",
       "1      0  [scorecard, best, lake, scene, leader, german,...   \n",
       "2      0  [coventry, caught, counter, flo, united, rival...   \n",
       "3      1  [pilgrim, know, progress, thatcher, facing, ba...   \n",
       "4      1  [hitches, horlocks, beckham, united, survive, ...   \n",
       "\n",
       "                                             doc_vec  \\\n",
       "0  [-0.13853689, -0.01842522, -0.011283746, 0.025...   \n",
       "1  [-0.13274541, -0.32202816, 0.17546004, 0.15300...   \n",
       "2  [-0.41247106, 0.4019597, 0.16694483, 0.0331079...   \n",
       "3  [-0.232915, -0.15494952, 0.13468608, -0.152680...   \n",
       "4  [-0.33469847, -0.034216043, 0.117651, -0.06944...   \n",
       "\n",
       "                                            news_str  \n",
       "0  operation extract leaked report scorecard hugh...  \n",
       "1  scorecard best lake scene leader german sleaze...  \n",
       "2  coventry caught counter flo united rival road ...  \n",
       "3  pilgrim know progress thatcher facing ban mcil...  \n",
       "4  hitches horlocks beckham united survive breast...  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('label_news_docvec_newsstr.pkl', 'rb') as r:\n",
    "    df = pickle.load(r)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763f0636",
   "metadata": {},
   "source": [
    "## Topic Modeling to vectors ( Unsupervised Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5b94ac",
   "metadata": {},
   "source": [
    "#### Data Allocation\n",
    "1. For Topic Modelling: 15% data set aside for testing. use the 85% for topic modelling\n",
    "2. Apply the topic model to the testing data to get the topic vectors\n",
    "3. Create the final train,valid, test files for AWS\n",
    "\n",
    "Topic Modeling Models\n",
    "1. Use HDP(Hierarchical Dirichlet Process) to decide topic size\n",
    "2. Use LDA (Latent Dirichlet Allocation to determine the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2a3e5317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3623 478\n"
     ]
    }
   ],
   "source": [
    "df_HDP_train = df.news.iloc[:3623]\n",
    "df_LDA_train = df.news_str.iloc[:3623]\n",
    "df_LDA_test = df.news_str.iloc[3623:]\n",
    "print(len(df_LDA_train), len(df_LDA_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad63851",
   "metadata": {},
   "source": [
    "## HDP corpus and dictionary (need to be bag of words format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8ff4df8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word_hdp = gensim.corpora.Dictionary(df_HDP_train)\n",
    "id2word_hdp.filter_extremes(no_below=10, no_above=0.30)\n",
    "id2word_hdp.compactify()\n",
    "id2word_hdp.save('train_dict_hdp')\n",
    "corpus_hdp = [id2word_hdp.doc2bow(doc) for doc in df_HDP_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60745cf7",
   "metadata": {},
   "source": [
    "## Use HDP model to decide the maxium topic numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bc4202bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import HdpModel\n",
    "hdp = HdpModel(corpus_hdp, id2word_hdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "123f45ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hdp.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e5a19d52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.004*israel + 0.004*china + 0.003*people + 0.003*country + 0.003*russia + 0.003*war + 0.003*america + 0.003*uk + 0.003*killed + 0.003*one'),\n",
       " (1,\n",
       "  '0.007*nan + 0.004*israel + 0.003*war + 0.003*people + 0.003*china + 0.003*israeli + 0.003*country + 0.003*one + 0.003*leader + 0.002*iran'),\n",
       " (2,\n",
       "  '0.003*rugby + 0.003*football + 0.003*letters + 0.003*review + 0.003*cup + 0.003*cricket + 0.002*england + 0.002*israel + 0.002*union + 0.002*leader'),\n",
       " (3,\n",
       "  '0.005*football + 0.005*review + 0.003*england + 0.003*cricket + 0.003*cd + 0.003*united + 0.002*cup + 0.002*back + 0.002*league + 0.002*rugby'),\n",
       " (4,\n",
       "  '0.006*nan + 0.004*review + 0.002*football + 0.002*united + 0.002*letters + 0.002*war + 0.002*england + 0.002*uk + 0.002*city + 0.002*face'),\n",
       " (5,\n",
       "  '0.007*nan + 0.003*review + 0.002*letters + 0.002*england + 0.002*london + 0.002*leader + 0.002*face + 0.002*rugby + 0.002*diary + 0.002*racing'),\n",
       " (6,\n",
       "  '0.004*nan + 0.001*back + 0.001*admits + 0.001*face + 0.001*attempting + 0.001*week + 0.001*drug + 0.001*war + 0.001*thought + 0.001*london'),\n",
       " (7,\n",
       "  '0.002*review + 0.002*city + 0.001*united + 0.001*make + 0.001*peace + 0.001*first + 0.001*man + 0.001*division + 0.001*tories + 0.001*deal'),\n",
       " (8,\n",
       "  '0.003*cd + 0.002*united + 0.001*city + 0.001*london + 0.001*week + 0.001*football + 0.001*anarchist + 0.001*spain + 0.001*england + 0.001*attack'),\n",
       " (9,\n",
       "  '0.003*review + 0.002*day + 0.001*united + 0.001*ambitious + 0.001*hit + 0.001*city + 0.001*cup + 0.001*theatre + 0.001*classical + 0.001*look'),\n",
       " (10,\n",
       "  '0.002*england + 0.001*face + 0.001*leader + 0.001*home + 0.001*blair + 0.001*day + 0.001*wwf + 0.001*review + 0.001*hutton + 0.001*take'),\n",
       " (11,\n",
       "  '0.002*england + 0.002*charge + 0.001*get + 0.001*football + 0.001*united + 0.001*go + 0.001*take + 0.001*pass + 0.001*women + 0.001*back'),\n",
       " (12,\n",
       "  '0.003*nan + 0.002*review + 0.001*school + 0.001*england + 0.001*writer + 0.001*drop + 0.001*tribe + 0.001*blair + 0.001*day + 0.001*right'),\n",
       " (13,\n",
       "  '0.006*nan + 0.001*blair + 0.001*england + 0.001*use + 0.001*london + 0.001*face + 0.001*win + 0.001*busted + 0.001*teacher + 0.001*merry'),\n",
       " (14,\n",
       "  '0.001*football + 0.001*take + 0.001*college + 0.001*england + 0.001*best + 0.001*benítez + 0.001*hostage + 0.001*house + 0.001*may + 0.001*pm'),\n",
       " (15,\n",
       "  '0.002*review + 0.001*aids + 0.001*guardian + 0.001*environment + 0.001*theatre + 0.001*vatican + 0.001*caribbean + 0.001*button + 0.001*ugly + 0.001*recognized'),\n",
       " (16,\n",
       "  '0.001*back + 0.001*five + 0.001*blair + 0.001*win + 0.001*algeria + 0.001*top + 0.001*chance + 0.001*failure + 0.001*deal + 0.001*benefits'),\n",
       " (17,\n",
       "  '0.002*cap + 0.001*diary + 0.001*review + 0.001*rock + 0.001*late + 0.001*decapitated + 0.001*leeds + 0.001*bnp + 0.001*nottingham + 0.001*england'),\n",
       " (18,\n",
       "  '0.001*student + 0.001*review + 0.001*week + 0.001*haaretz + 0.001*voice + 0.001*lies + 0.001*amsterdam + 0.001*tayyip + 0.001*back + 0.001*houllier'),\n",
       " (19,\n",
       "  '0.002*review + 0.001*england + 0.001*player + 0.001*urged + 0.001*serena + 0.001*may + 0.001*oct + 0.001*bolton + 0.001*fund + 0.001*united')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdp.print_topics(num_topics=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6200261",
   "metadata": {},
   "source": [
    "## LDA Modeling for topic vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e9d8a65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "# we need to have more stopwords for topic modeling than for word2vec.\n",
    "# NLTK + SKlearn + self definded\n",
    "sk_stop = list(_stop_words.ENGLISH_STOP_WORDS)\n",
    "mywords = ['whilst', 'say', 'says', 'today','yesterday', 'news', 'tomorrow','iii', 'ii', 'like', 'ha','wa']\n",
    "final_stop = stopwords.words('english') + mywords + sk_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3b25ed6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfv = TfidfVectorizer(stop_words = final_stop, ngram_range = (1, 2), max_df = 0.95)\n",
    "doc_word = tfv.fit_transform(df_LDA_train).transpose()\n",
    "corpus = matutils.Sparse2Corpus(doc_word)\n",
    "id2word = dict((v, k) for k, v in tfv.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3d6e91b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7bd63c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(filename='lda_model.log', format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    lda = LdaModel(corpus=corpus, num_topics=20, id2word=id2word, passes=5, random_state = 200 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9a514f86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.000*\"world\" + 0.000*\"iraq\" + 0.000*\"war\" + 0.000*\"new\" + 0.000*\"england\" + 0.000*\"cup\" + 0.000*\"report\" + 0.000*\"review\" + 0.000*\"united\" + 0.000*\"blair\"'),\n",
       " (1,\n",
       "  '0.000*\"football\" + 0.000*\"cd\" + 0.000*\"united\" + 0.000*\"rugby\" + 0.000*\"cricket\" + 0.000*\"new\" + 0.000*\"horse\" + 0.000*\"england\" + 0.000*\"cup\" + 0.000*\"racing\"'),\n",
       " (2,\n",
       "  '0.000*\"review\" + 0.000*\"new\" + 0.000*\"win\" + 0.000*\"football\" + 0.000*\"letters\" + 0.000*\"england\" + 0.000*\"league\" + 0.000*\"cricket\" + 0.000*\"power\" + 0.000*\"london\"'),\n",
       " (3,\n",
       "  '0.000*\"review\" + 0.000*\"league\" + 0.000*\"apr\" + 0.000*\"england\" + 0.000*\"football\" + 0.000*\"letters\" + 0.000*\"new\" + 0.000*\"rugby\" + 0.000*\"weekly\" + 0.000*\"face\"'),\n",
       " (4,\n",
       "  '0.000*\"england\" + 0.000*\"premiership\" + 0.000*\"cup\" + 0.000*\"review\" + 0.000*\"new\" + 0.000*\"united\" + 0.000*\"football\" + 0.000*\"fa\" + 0.000*\"best\" + 0.000*\"league\"'),\n",
       " (5,\n",
       "  '0.000*\"apr\" + 0.000*\"review\" + 0.000*\"film review\" + 0.000*\"film\" + 0.000*\"united\" + 0.000*\"england\" + 0.000*\"football\" + 0.000*\"new\" + 0.000*\"london\" + 0.000*\"obituary\"'),\n",
       " (6,\n",
       "  '0.000*\"premier league\" + 0.000*\"league\" + 0.000*\"cup\" + 0.000*\"premier\" + 0.000*\"england\" + 0.000*\"review\" + 0.000*\"cd\" + 0.000*\"football\" + 0.000*\"rugby\" + 0.000*\"obituary\"'),\n",
       " (7,\n",
       "  '0.000*\"review\" + 0.000*\"football\" + 0.000*\"world\" + 0.000*\"cup\" + 0.000*\"theatre\" + 0.000*\"obituary\" + 0.000*\"letters\" + 0.000*\"league\" + 0.000*\"division\" + 0.000*\"england\"'),\n",
       " (8,\n",
       "  '0.000*\"pick week\" + 0.000*\"korea\" + 0.000*\"north korea\" + 0.000*\"police\" + 0.000*\"new\" + 0.000*\"world\" + 0.000*\"north\" + 0.000*\"year\" + 0.000*\"government\" + 0.000*\"china\"'),\n",
       " (9,\n",
       "  '0.001*\"new\" + 0.001*\"israel\" + 0.001*\"world\" + 0.001*\"year\" + 0.001*\"china\" + 0.001*\"government\" + 0.001*\"police\" + 0.001*\"review\" + 0.001*\"people\" + 0.001*\"war\"'),\n",
       " (10,\n",
       "  '0.000*\"review\" + 0.000*\"cricket\" + 0.000*\"england\" + 0.000*\"football\" + 0.000*\"cup\" + 0.000*\"theatre\" + 0.000*\"new\" + 0.000*\"leader\" + 0.000*\"london\" + 0.000*\"manchester\"'),\n",
       " (11,\n",
       "  '0.000*\"israel\" + 0.000*\"israeli\" + 0.000*\"new\" + 0.000*\"woman\" + 0.000*\"year\" + 0.000*\"country\" + 0.000*\"attack\" + 0.000*\"review\" + 0.000*\"london\" + 0.000*\"death\"'),\n",
       " (12,\n",
       "  '0.000*\"bin\" + 0.000*\"bin laden\" + 0.000*\"laden\" + 0.000*\"review\" + 0.000*\"new\" + 0.000*\"world\" + 0.000*\"cd\" + 0.000*\"year\" + 0.000*\"war\" + 0.000*\"north\"'),\n",
       " (13,\n",
       "  '0.000*\"review\" + 0.000*\"rugby\" + 0.000*\"cd\" + 0.000*\"cup\" + 0.000*\"england\" + 0.000*\"cricket\" + 0.000*\"win\" + 0.000*\"new\" + 0.000*\"world\" + 0.000*\"diary\"'),\n",
       " (14,\n",
       "  '0.000*\"football\" + 0.000*\"review\" + 0.000*\"new\" + 0.000*\"letters\" + 0.000*\"day\" + 0.000*\"police\" + 0.000*\"man\" + 0.000*\"jun\" + 0.000*\"people\" + 0.000*\"world\"'),\n",
       " (15,\n",
       "  '0.000*\"ukraine\" + 0.000*\"pakistan\" + 0.000*\"korea\" + 0.000*\"cup qualifier\" + 0.000*\"new\" + 0.000*\"qualifier\" + 0.000*\"world\" + 0.000*\"israel\" + 0.000*\"chinese\" + 0.000*\"people\"'),\n",
       " (16,\n",
       "  '0.000*\"cd\" + 0.000*\"review\" + 0.000*\"football\" + 0.000*\"new\" + 0.000*\"rugby\" + 0.000*\"cup\" + 0.000*\"obituary\" + 0.000*\"letters\" + 0.000*\"league\" + 0.000*\"england\"'),\n",
       " (17,\n",
       "  '0.000*\"football\" + 0.000*\"cd\" + 0.000*\"review\" + 0.000*\"letters\" + 0.000*\"england\" + 0.000*\"ukraine\" + 0.000*\"london\" + 0.000*\"united\" + 0.000*\"leader\" + 0.000*\"world\"'),\n",
       " (18,\n",
       "  '0.000*\"china\" + 0.000*\"russia\" + 0.000*\"year\" + 0.000*\"world\" + 0.000*\"iran\" + 0.000*\"government\" + 0.000*\"country\" + 0.000*\"police\" + 0.000*\"new\" + 0.000*\"war\"'),\n",
       " (19,\n",
       "  '0.000*\"review\" + 0.000*\"new\" + 0.000*\"england\" + 0.000*\"cd\" + 0.000*\"football\" + 0.000*\"diary\" + 0.000*\"rugby\" + 0.000*\"make\" + 0.000*\"best\" + 0.000*\"cup\"')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d0623b",
   "metadata": {},
   "source": [
    "## Make Topic Vectors\n",
    "\n",
    "# Train Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a196f65a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 3623)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(len(df_LDA_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e1933f22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3623"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4214dd48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3623\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.014720522,\n",
       " 0.014720522,\n",
       " 0.014720522,\n",
       " 0.014720522,\n",
       " 0.014720522,\n",
       " 0.014720522,\n",
       " 0.014720522,\n",
       " 0.014720522,\n",
       " 0.014720522,\n",
       " 0.7203101,\n",
       " 0.014720522,\n",
       " 0.014720522,\n",
       " 0.014720522,\n",
       " 0.014720522,\n",
       " 0.014720522,\n",
       " 0.014720522,\n",
       " 0.014720522,\n",
       " 0.014720522,\n",
       " 0.014720522,\n",
       " 0.014720522]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_vecs_train = []\n",
    "for i in range(len(df_LDA_train)):\n",
    "    doc_topics = lda.get_document_topics(corpus[i], minimum_probability=0.0)\n",
    "    doc_top_vec = [doc_topics[num][1] for num in range(20)]\n",
    "    top_vecs_train.append(doc_top_vec)\n",
    "\n",
    "print(len(top_vecs_train))\n",
    "top_vecs_train[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75427b0e",
   "metadata": {},
   "source": [
    "## TEST Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8756a8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "478\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.049999993,\n",
       " 0.049999993,\n",
       " 0.049999993,\n",
       " 0.049999993,\n",
       " 0.049999993,\n",
       " 0.049999993,\n",
       " 0.049999993,\n",
       " 0.049999993,\n",
       " 0.049999993,\n",
       " 0.05000013,\n",
       " 0.049999993,\n",
       " 0.049999993,\n",
       " 0.049999993,\n",
       " 0.049999993,\n",
       " 0.049999993,\n",
       " 0.049999993,\n",
       " 0.049999993,\n",
       " 0.049999993,\n",
       " 0.049999993,\n",
       " 0.049999993]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the topic model from train data to get get data topic vectors\n",
    "doc_word_test = tfv.fit_transform(df_LDA_test).transpose()\n",
    "corpus_test = matutils.Sparse2Corpus(doc_word_test)\n",
    "\n",
    "top_vecs_test = []\n",
    "for i in range(len(df_LDA_test)):\n",
    "    doc_topics_test = lda.get_document_topics(corpus_test[i], minimum_probability=0.0)\n",
    "    doc_top_vec_test = [doc_topics_test[num][1] for num in range(20)]\n",
    "    top_vecs_test.append(doc_top_vec_test)\n",
    "\n",
    "print(len(top_vecs_test))\n",
    "top_vecs_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f84ab297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the two lists then convert to a Seires adding to the full dataframe for train, valid, test split.\n",
    "top_vecs = pd.Series(top_vecs_train + top_vecs_test, name = 'top_vecs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "249f438a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>news</th>\n",
       "      <th>doc_vec</th>\n",
       "      <th>news_str</th>\n",
       "      <th>top_vecs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[operation, extract, leaked, report, scorecard...</td>\n",
       "      <td>[-0.13853689, -0.01842522, -0.011283746, 0.025...</td>\n",
       "      <td>operation extract leaked report scorecard hugh...</td>\n",
       "      <td>[0.016551204, 0.016551204, 0.016551204, 0.0165...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[scorecard, best, lake, scene, leader, german,...</td>\n",
       "      <td>[-0.13274541, -0.32202816, 0.17546004, 0.15300...</td>\n",
       "      <td>scorecard best lake scene leader german sleaze...</td>\n",
       "      <td>[0.018115353, 0.018115353, 0.018115353, 0.0181...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[coventry, caught, counter, flo, united, rival...</td>\n",
       "      <td>[-0.41247106, 0.4019597, 0.16694483, 0.0331079...</td>\n",
       "      <td>coventry caught counter flo united rival road ...</td>\n",
       "      <td>[0.015861943, 0.015861943, 0.015861943, 0.0158...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>[pilgrim, know, progress, thatcher, facing, ba...</td>\n",
       "      <td>[-0.232915, -0.15494952, 0.13468608, -0.152680...</td>\n",
       "      <td>pilgrim know progress thatcher facing ban mcil...</td>\n",
       "      <td>[0.014361328, 0.014361328, 0.014361328, 0.0143...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>[hitches, horlocks, beckham, united, survive, ...</td>\n",
       "      <td>[-0.33469847, -0.034216043, 0.117651, -0.06944...</td>\n",
       "      <td>hitches horlocks beckham united survive breast...</td>\n",
       "      <td>[0.018412992, 0.018412992, 0.018412992, 0.0184...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label                                               news  \\\n",
       "0      0  [operation, extract, leaked, report, scorecard...   \n",
       "1      0  [scorecard, best, lake, scene, leader, german,...   \n",
       "2      0  [coventry, caught, counter, flo, united, rival...   \n",
       "3      1  [pilgrim, know, progress, thatcher, facing, ba...   \n",
       "4      1  [hitches, horlocks, beckham, united, survive, ...   \n",
       "\n",
       "                                             doc_vec  \\\n",
       "0  [-0.13853689, -0.01842522, -0.011283746, 0.025...   \n",
       "1  [-0.13274541, -0.32202816, 0.17546004, 0.15300...   \n",
       "2  [-0.41247106, 0.4019597, 0.16694483, 0.0331079...   \n",
       "3  [-0.232915, -0.15494952, 0.13468608, -0.152680...   \n",
       "4  [-0.33469847, -0.034216043, 0.117651, -0.06944...   \n",
       "\n",
       "                                            news_str  \\\n",
       "0  operation extract leaked report scorecard hugh...   \n",
       "1  scorecard best lake scene leader german sleaze...   \n",
       "2  coventry caught counter flo united rival road ...   \n",
       "3  pilgrim know progress thatcher facing ban mcil...   \n",
       "4  hitches horlocks beckham united survive breast...   \n",
       "\n",
       "                                            top_vecs  \n",
       "0  [0.016551204, 0.016551204, 0.016551204, 0.0165...  \n",
       "1  [0.018115353, 0.018115353, 0.018115353, 0.0181...  \n",
       "2  [0.015861943, 0.015861943, 0.015861943, 0.0158...  \n",
       "3  [0.014361328, 0.014361328, 0.014361328, 0.0143...  \n",
       "4  [0.018412992, 0.018412992, 0.018412992, 0.0184...  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['top_vecs'] = top_vecs\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "756a0e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n",
      "4101\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>news</th>\n",
       "      <th>doc_vec</th>\n",
       "      <th>news_str</th>\n",
       "      <th>top_vecs</th>\n",
       "      <th>vectors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[operation, extract, leaked, report, scorecard...</td>\n",
       "      <td>[-0.13853689, -0.01842522, -0.011283746, 0.025...</td>\n",
       "      <td>operation extract leaked report scorecard hugh...</td>\n",
       "      <td>[0.016551204, 0.016551204, 0.016551204, 0.0165...</td>\n",
       "      <td>[-0.13853689, -0.01842522, -0.011283746, 0.025...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[scorecard, best, lake, scene, leader, german,...</td>\n",
       "      <td>[-0.13274541, -0.32202816, 0.17546004, 0.15300...</td>\n",
       "      <td>scorecard best lake scene leader german sleaze...</td>\n",
       "      <td>[0.018115353, 0.018115353, 0.018115353, 0.0181...</td>\n",
       "      <td>[-0.13274541, -0.32202816, 0.17546004, 0.15300...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[coventry, caught, counter, flo, united, rival...</td>\n",
       "      <td>[-0.41247106, 0.4019597, 0.16694483, 0.0331079...</td>\n",
       "      <td>coventry caught counter flo united rival road ...</td>\n",
       "      <td>[0.015861943, 0.015861943, 0.015861943, 0.0158...</td>\n",
       "      <td>[-0.41247106, 0.4019597, 0.16694483, 0.0331079...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>[pilgrim, know, progress, thatcher, facing, ba...</td>\n",
       "      <td>[-0.232915, -0.15494952, 0.13468608, -0.152680...</td>\n",
       "      <td>pilgrim know progress thatcher facing ban mcil...</td>\n",
       "      <td>[0.014361328, 0.014361328, 0.014361328, 0.0143...</td>\n",
       "      <td>[-0.232915, -0.15494952, 0.13468608, -0.152680...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>[hitches, horlocks, beckham, united, survive, ...</td>\n",
       "      <td>[-0.33469847, -0.034216043, 0.117651, -0.06944...</td>\n",
       "      <td>hitches horlocks beckham united survive breast...</td>\n",
       "      <td>[0.018412992, 0.018412992, 0.018412992, 0.0184...</td>\n",
       "      <td>[-0.33469847, -0.034216043, 0.117651, -0.06944...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label                                               news  \\\n",
       "0      0  [operation, extract, leaked, report, scorecard...   \n",
       "1      0  [scorecard, best, lake, scene, leader, german,...   \n",
       "2      0  [coventry, caught, counter, flo, united, rival...   \n",
       "3      1  [pilgrim, know, progress, thatcher, facing, ba...   \n",
       "4      1  [hitches, horlocks, beckham, united, survive, ...   \n",
       "\n",
       "                                             doc_vec  \\\n",
       "0  [-0.13853689, -0.01842522, -0.011283746, 0.025...   \n",
       "1  [-0.13274541, -0.32202816, 0.17546004, 0.15300...   \n",
       "2  [-0.41247106, 0.4019597, 0.16694483, 0.0331079...   \n",
       "3  [-0.232915, -0.15494952, 0.13468608, -0.152680...   \n",
       "4  [-0.33469847, -0.034216043, 0.117651, -0.06944...   \n",
       "\n",
       "                                            news_str  \\\n",
       "0  operation extract leaked report scorecard hugh...   \n",
       "1  scorecard best lake scene leader german sleaze...   \n",
       "2  coventry caught counter flo united rival road ...   \n",
       "3  pilgrim know progress thatcher facing ban mcil...   \n",
       "4  hitches horlocks beckham united survive breast...   \n",
       "\n",
       "                                            top_vecs  \\\n",
       "0  [0.016551204, 0.016551204, 0.016551204, 0.0165...   \n",
       "1  [0.018115353, 0.018115353, 0.018115353, 0.0181...   \n",
       "2  [0.015861943, 0.015861943, 0.015861943, 0.0158...   \n",
       "3  [0.014361328, 0.014361328, 0.014361328, 0.0143...   \n",
       "4  [0.018412992, 0.018412992, 0.018412992, 0.0184...   \n",
       "\n",
       "                                             vectors  \n",
       "0  [-0.13853689, -0.01842522, -0.011283746, 0.025...  \n",
       "1  [-0.13274541, -0.32202816, 0.17546004, 0.15300...  \n",
       "2  [-0.41247106, 0.4019597, 0.16694483, 0.0331079...  \n",
       "3  [-0.232915, -0.15494952, 0.13468608, -0.152680...  \n",
       "4  [-0.33469847, -0.034216043, 0.117651, -0.06944...  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['vectors'] = pd.Series([\n",
    "    (list(df.doc_vec[row]) if not isinstance(df.doc_vec[row], float) else [df.doc_vec[row]]) +\n",
    "    (list(df.top_vecs[row]) if not isinstance(df.top_vecs[row], float) else [df.top_vecs[row]])\n",
    "    for row in range(len(df))\n",
    "])\n",
    "\n",
    "print(len(df.vectors[0]))\n",
    "print(len(df.vectors))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88070822",
   "metadata": {},
   "source": [
    "## Modeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ccaa33ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import  svm, naive_bayes, neighbors, ensemble\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
    "\n",
    "lr_model = LogisticRegression()\n",
    "nb_model = naive_bayes.GaussianNB()\n",
    "knn_model = neighbors.KNeighborsClassifier()\n",
    "svc_model = svm.SVC(probability=True, gamma=\"scale\")\n",
    "rf_model = ensemble.RandomForestClassifier(n_estimators=100)\n",
    "et_model = ensemble.ExtraTreesClassifier(n_estimators=100)\n",
    "ada_model = ensemble.AdaBoostClassifier()\n",
    "xgb_model = xgb.XGBClassifier(max_depth=50, n_estimators=80, learning_rate=0.1, colsample_bytree=.7, gamma=0, \n",
    "                              reg_alpha=4, objective='binary:logistic', eta=0.3, silent=1, subsample=0.8)\n",
    "\n",
    "models = [\"lr_model\", \"nb_model\", \"knn_model\", \"svc_model\", \"rf_model\", \"et_model\", \"ada_model\", \"xgb_model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "60b8f782",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model_filter(modellist, X, y):\n",
    "    ''' 1. split the train data further into train and validation (17%). \n",
    "        2. fit the train data into each model of the model list\n",
    "        3. get the classification report based on the model performance on validation data\n",
    "    '''\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X,y, test_size = 0.17, random_state = 100)\n",
    "    for model_name in modellist:\n",
    "        curr_model = eval(model_name)\n",
    "        curr_model.fit(X_train, y_train) \n",
    "        print(f'{model_name} \\n report:{classification_report(y_valid, curr_model.predict(X_valid))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2c69448a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('modified4.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7909dd9f",
   "metadata": {},
   "source": [
    "## doc2vec + topic vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "de939548",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harsh\\anaconda3\\envs\\mygpu\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr_model \n",
      " report:              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.45      0.49       142\n",
      "           1       0.54      0.62      0.58       145\n",
      "\n",
      "    accuracy                           0.54       287\n",
      "   macro avg       0.54      0.54      0.53       287\n",
      "weighted avg       0.54      0.54      0.53       287\n",
      "\n",
      "nb_model \n",
      " report:              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.68      0.56       142\n",
      "           1       0.48      0.29      0.36       145\n",
      "\n",
      "    accuracy                           0.48       287\n",
      "   macro avg       0.48      0.48      0.46       287\n",
      "weighted avg       0.48      0.48      0.46       287\n",
      "\n",
      "knn_model \n",
      " report:              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.44      0.46       142\n",
      "           1       0.51      0.57      0.53       145\n",
      "\n",
      "    accuracy                           0.50       287\n",
      "   macro avg       0.50      0.50      0.50       287\n",
      "weighted avg       0.50      0.50      0.50       287\n",
      "\n",
      "svc_model \n",
      " report:              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.35      0.42       142\n",
      "           1       0.51      0.67      0.58       145\n",
      "\n",
      "    accuracy                           0.51       287\n",
      "   macro avg       0.51      0.51      0.50       287\n",
      "weighted avg       0.51      0.51      0.50       287\n",
      "\n",
      "rf_model \n",
      " report:              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.46      0.49       142\n",
      "           1       0.52      0.56      0.54       145\n",
      "\n",
      "    accuracy                           0.51       287\n",
      "   macro avg       0.51      0.51      0.51       287\n",
      "weighted avg       0.51      0.51      0.51       287\n",
      "\n",
      "et_model \n",
      " report:              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.47      0.49       142\n",
      "           1       0.51      0.54      0.53       145\n",
      "\n",
      "    accuracy                           0.51       287\n",
      "   macro avg       0.51      0.51      0.51       287\n",
      "weighted avg       0.51      0.51      0.51       287\n",
      "\n",
      "ada_model \n",
      " report:              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.42      0.44       142\n",
      "           1       0.48      0.52      0.50       145\n",
      "\n",
      "    accuracy                           0.47       287\n",
      "   macro avg       0.47      0.47      0.47       287\n",
      "weighted avg       0.47      0.47      0.47       287\n",
      "\n",
      "[15:50:44] WARNING: C:\\Users\\dev-admin\\croot2\\xgboost-split_1675461376218\\work\\src\\learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "xgb_model \n",
      " report:              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.44      0.45       142\n",
      "           1       0.48      0.50      0.49       145\n",
      "\n",
      "    accuracy                           0.47       287\n",
      "   macro avg       0.47      0.47      0.47       287\n",
      "weighted avg       0.47      0.47      0.47       287\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = np.array(list(df.vectors[:1688]))\n",
    "y = np.array(list(df.Label[:1688]))\n",
    "\n",
    "baseline_model_filter(models, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a6bce4",
   "metadata": {},
   "source": [
    "## topic vectors only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6ba43490",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harsh\\anaconda3\\envs\\mygpu\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\harsh\\anaconda3\\envs\\mygpu\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\harsh\\anaconda3\\envs\\mygpu\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr_model \n",
      " report:              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       142\n",
      "           1       0.51      1.00      0.67       145\n",
      "\n",
      "    accuracy                           0.51       287\n",
      "   macro avg       0.25      0.50      0.34       287\n",
      "weighted avg       0.26      0.51      0.34       287\n",
      "\n",
      "nb_model \n",
      " report:              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.80      0.61       142\n",
      "           1       0.49      0.19      0.28       145\n",
      "\n",
      "    accuracy                           0.49       287\n",
      "   macro avg       0.49      0.49      0.44       287\n",
      "weighted avg       0.49      0.49      0.44       287\n",
      "\n",
      "knn_model \n",
      " report:              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.43      0.44       142\n",
      "           1       0.47      0.49      0.48       145\n",
      "\n",
      "    accuracy                           0.46       287\n",
      "   macro avg       0.46      0.46      0.46       287\n",
      "weighted avg       0.46      0.46      0.46       287\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harsh\\anaconda3\\envs\\mygpu\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\harsh\\anaconda3\\envs\\mygpu\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\harsh\\anaconda3\\envs\\mygpu\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svc_model \n",
      " report:              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       142\n",
      "           1       0.51      1.00      0.67       145\n",
      "\n",
      "    accuracy                           0.51       287\n",
      "   macro avg       0.25      0.50      0.34       287\n",
      "weighted avg       0.26      0.51      0.34       287\n",
      "\n",
      "rf_model \n",
      " report:              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.51      0.50       142\n",
      "           1       0.51      0.50      0.50       145\n",
      "\n",
      "    accuracy                           0.50       287\n",
      "   macro avg       0.50      0.50      0.50       287\n",
      "weighted avg       0.50      0.50      0.50       287\n",
      "\n",
      "et_model \n",
      " report:              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.51      0.50       142\n",
      "           1       0.50      0.49      0.50       145\n",
      "\n",
      "    accuracy                           0.50       287\n",
      "   macro avg       0.50      0.50      0.50       287\n",
      "weighted avg       0.50      0.50      0.50       287\n",
      "\n",
      "ada_model \n",
      " report:              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.50      0.50       142\n",
      "           1       0.51      0.51      0.51       145\n",
      "\n",
      "    accuracy                           0.51       287\n",
      "   macro avg       0.51      0.51      0.51       287\n",
      "weighted avg       0.51      0.51      0.51       287\n",
      "\n",
      "[15:51:41] WARNING: C:\\Users\\dev-admin\\croot2\\xgboost-split_1675461376218\\work\\src\\learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "xgb_model \n",
      " report:              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.42      0.44       142\n",
      "           1       0.48      0.53      0.51       145\n",
      "\n",
      "    accuracy                           0.48       287\n",
      "   macro avg       0.48      0.48      0.48       287\n",
      "weighted avg       0.48      0.48      0.48       287\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = np.array(list(df.top_vecs[:1688]))\n",
    "y = np.array(list(df.Label[:1688]))\n",
    "\n",
    "baseline_model_filter(models, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8a5b6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
